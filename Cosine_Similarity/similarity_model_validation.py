# -*- coding: utf-8 -*-
"""Similarity_Model_for_Matching_Developer_Profiles_with_Task_Descriptionsipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JFbzhGlAqcmJO1Pfule7AUbmwu71dNCN

**Mount Google Drive**

First, you'll need to mount your Google Drive to access the dataset
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Install Required Libraries**

"""

!pip install -q sentence-transformers scipy pandas matplotlib seaborn

"""**Load the Dataset from Google Drive**



"""

import pandas as pd

# Load your dataset from Google Drive
df = pd.read_csv('/content/drive/MyDrive/Cosine_similarity_update.csv')

# Display the first few rows to ensure it's loaded correctly
df.head()

"""**Check and Clean Column Names**

For Verification of column names to avoid the KeyError:
"""

# Clean column names
df.columns = df.columns.str.strip()
print(df.columns)

"""**Dropping Missing Values in Text Columns**"""

# Drop rows where either description is missing
df = df.dropna(subset=['task_description', 'member_description'])

# Preview to confirm
df[['task_description', 'member_description']].head()

"""**Install Sentence Transformers**


"""

!pip install -q sentence-transformers
from sentence_transformers import SentenceTransformer, util

"""**Loading the SBERT Model**

"""

# Load SBERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Encode the task descriptions and member descriptions
task_embeddings = model.encode(df['task_description'].tolist(), convert_to_tensor=True)
member_embeddings = model.encode(df['member_description'].tolist(), convert_to_tensor=True)

"""**Calculate Cosine Similarity**

Now letâ€™s calculate the cosine similarity between each pair of task_description and member_description embeddings.
"""

# Calculate cosine similarity
cosine_similarities = util.cos_sim(task_embeddings, member_embeddings)

# Since it's a square matrix, get the diagonal (similarity of each pair)
similarity_scores = cosine_similarities.diagonal().cpu().numpy()

# Add these scores to the dataframe
df['cosine_similarity'] = similarity_scores

# Display the top 5 results
df[['task_description', 'member_description', 'cosine_similarity']].head()

"""**Compare with Ground Truth**


"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Set threshold for classification
threshold = 0.5
predicted_matches = (df['cosine_similarity'] >= threshold).astype(int)

# True labels from the dataset
true_matches = df['task_member_description_is_match']

# Calculate metrics
accuracy = accuracy_score(true_matches, predicted_matches)
precision = precision_score(true_matches, predicted_matches)
recall = recall_score(true_matches, predicted_matches)
f1 = f1_score(true_matches, predicted_matches)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

"""**Analyzing the cosine similarity distribution**

Plot a histogram to see the score spread.
"""

import matplotlib.pyplot as plt

plt.hist(df['cosine_similarity'], bins=50)
plt.title('Distribution of Cosine Similarities')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.show()

"""**Optimizing the Threshold**

Try different thresholds and pick the one with the best F1-score.
"""

import numpy as np

best_f1 = 0
best_threshold = 0
for t in np.arange(0.01, 0.5, 0.01):
    preds = (df['cosine_similarity'] >= t).astype(int)
    f1 = f1_score(df['task_member_description_is_match'], preds)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = t

print(f"Best Threshold: {best_threshold:.2f}, Best F1-Score: {best_f1:.4f}")

"""**Applying the best threshold and see the new performance metrics:**"""

from sklearn.metrics import classification_report

# Apply the best threshold
df['predicted_match'] = (df['cosine_similarity'] >= 0.04).astype(int)

# Show new metrics
print(classification_report(df['task_member_description_is_match'], df['predicted_match']))

"""**Visualizing True Positives**"""

matches = df[df['predicted_match'] == 1]
print(matches[['task_description', 'member_description', 'cosine_similarity']].head(5))

"""**Visualize the distribution of cosine similarity values**"""

import matplotlib.pyplot as plt

# Plot histogram of cosine similarities
plt.hist(df['cosine_similarity'], bins=20, edgecolor='black')
plt.title('Cosine Similarity Distribution')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.show()

"""**Installing and loading SBERT**"""

pip install sentence-transformers

from sentence_transformers import SentenceTransformer

# Load pre-trained SBERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

"""**Generating Embeddings with SBERT**"""

task_embeddings = model.encode(df['task_description'].tolist(), convert_to_tensor=True)
member_embeddings = model.encode(df['member_description'].tolist(), convert_to_tensor=True)

"""**Calculate Cosine Similarity with SBERT**"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_similarities = cosine_similarity(task_embeddings, member_embeddings)
df['cosine_similarity_sbert'] = cosine_similarities.diagonal()

y_true = df['task_member_description_is_match']

"""**Final Hypermeter Model Tuning**"""

from sklearn.metrics import f1_score

# Define the true labels
y_true = df['task_member_description_is_match']

# Experiment with different thresholds
thresholds = [0.05, 0.1, 0.15, 0.2]
for threshold in thresholds:
    y_pred = (df['cosine_similarity_sbert'] >= threshold).astype(int)
    f1 = f1_score(y_true, y_pred)
    print(f"Threshold: {threshold}, F1-Score: {f1}")

import pandas as pd

# Creating a larger sample ground truth dataset
data = {
    'task_description': [
        "Develop a full-stack web application using React and Node.js",
        "Create a machine learning model to predict house prices",
        "Develop a mobile app using Flutter",
        "Write a Python script to automate data processing tasks",
        "Create a complex SQL database for a large-scale application",
        "Develop a RESTful API with Python and Django",
        "Build a data pipeline for real-time data streaming",
        "Create a website with HTML, CSS, and JavaScript",
        "Write a shell script for file automation tasks",
        "Design a user-friendly mobile app interface",
        "Build a machine learning model using TensorFlow",
        "Create a database schema for an e-commerce platform",
        "Develop a chatbot using natural language processing",
        "Write unit tests for an existing web application",
        "Design and develop an online payment gateway",
        "Build a recommendation engine using collaborative filtering",
        "Implement a search algorithm for large datasets",
        "Design an interactive dashboard using Tableau",
        "Create a data visualization tool with Python and Matplotlib",
        "Develop a content management system (CMS) using PHP"
    ],
    'developer_profile': [
        "I am a full-stack web developer, skilled in React and Node.js",
        "I am a software engineer with a focus on front-end development and UI design",
        "I am a mobile developer experienced with Flutter and Dart",
        "I am an experienced Python developer with expertise in data analysis",
        "I am a front-end developer with basic SQL knowledge",
        "Experienced in building REST APIs with Python and Django",
        "I am a data engineer with experience in building data pipelines",
        "Proficient in HTML, CSS, JavaScript, and responsive design",
        "I have written multiple shell scripts to automate system tasks",
        "I specialize in UI/UX design for mobile apps using Figma",
        "I am a machine learning engineer with experience in TensorFlow",
        "I have designed databases and schemas for e-commerce applications",
        "I have built conversational bots using NLP frameworks like Rasa",
        "I write unit tests using frameworks like PyTest and unittest",
        "I have designed and integrated secure payment gateways in web apps",
        "I have built recommendation systems using collaborative filtering",
        "I have implemented search algorithms like binary search and hash tables",
        "I specialize in designing dashboards and data visualizations using Tableau",
        "I have developed various data visualization tools using Python and Matplotlib",
        "I have built CMS platforms using PHP and MySQL"
    ],
    'match_label': [
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0
    ]
}

# Create a DataFrame
df = pd.DataFrame(data)

# Save as a CSV file
df.to_csv('ground_truth_sample_large.csv', index=False)

# Display the DataFrame to confirm
print(df.head())  # Displaying first 5 rows to check

import os

# Create the directory if it doesn't exist
directory = '/content/drive/MyDrive/GroundTruthData'
if not os.path.exists(directory):
    os.makedirs(directory)

# Save the CSV file to the new directory
df.to_csv(f'{directory}/ground_truth_sample_large.csv', index=False)

# Confirm by printing the file path
print(f"File saved to Google Drive at {directory}")

# Save the CSV file directly to the root of Google Drive
df.to_csv('/content/drive/MyDrive/ground_truth_sample_large.csv', index=False)

# Confirm by printing the file path
print("File saved to Google Drive at /content/drive/MyDrive/")

import pandas as pd

# Load the dataset
file_path = '/content/drive/MyDrive/ground_truth_sample_large.csv'
df = pd.read_csv(file_path)

# Display the first few rows of the dataset to verify
df.head()

!pip install sentence-transformers

# Check the column names in your dataframe
print(df.columns)

from sentence_transformers import SentenceTransformer

# Load pre-trained SBERT model
model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose a different SBERT model

# Generate embeddings for task descriptions and member descriptions or skills
df['task_description_embedding'] = df['task_description'].apply(lambda x: model.encode(x))

# Choose either 'member_description' or 'member_skills' based on the context
df['developer_profile_embedding'] = df['member_description'].apply(lambda x: model.encode(x))  # Or use 'member_skills'

# Display the dataframe with embeddings
df.head()

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Calculate the cosine similarity between task descriptions and developer profiles
df['cosine_similarity_sbert'] = df.apply(
    lambda row: cosine_similarity([row['task_description_embedding']], [row['developer_profile_embedding']])[0][0], axis=1
)

# Set a threshold to determine the match (e.g., 0.8)
threshold = 0.8
df['predicted_match'] = df['cosine_similarity_sbert'].apply(lambda x: 1 if x >= threshold else 0)

# Display the updated DataFrame
df[['challengeId', 'task_description', 'member_description', 'cosine_similarity_sbert', 'predicted_match']].head()

threshold = 0.2  # Adjust the threshold value
df['predicted_match'] = df['cosine_similarity_sbert'].apply(lambda x: 1 if x >= threshold else 0)

print(df[['challengeId', 'task_description', 'member_description']].head())

import re

def clean_text(text):
    # Remove special characters, URLs, and extra spaces
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove non-alphanumeric characters
    return text

df['task_description'] = df['task_description'].apply(clean_text)
df['member_description'] = df['member_description'].apply(clean_text)

import re

def clean_text(text):
    # Remove special characters, URLs, and extra spaces
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove non-alphanumeric characters
    return text

df['task_description'] = df['task_description'].apply(clean_text)
df['member_description'] = df['member_description'].apply(clean_text)

print(df.columns)

from sentence_transformers import SentenceTransformer

# Load the pre-trained model
model = SentenceTransformer('all-MiniLM-L6-v2')  # You can use any other model if needed

# Generate embeddings for 'member_description'
df['member_description_embedding'] = df['member_description'].apply(lambda x: model.encode(x))

# Check if the embedding column is added successfully
print(df.head())

from sklearn.metrics.pairwise import cosine_similarity

# Now calculate the cosine similarity
df['cosine_similarity_sbert'] = df.apply(
    lambda row: cosine_similarity([row['task_description_embedding']], [row['member_description_embedding']])[0][0],
    axis=1
)

# Check the updated DataFrame
print(df.head())

import matplotlib.pyplot as plt

# Plot the cosine similarity scores
plt.hist(df['cosine_similarity_sbert'], bins=20, edgecolor='black')
plt.title('Distribution of Cosine Similarity Scores')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.show()

# Filter out the rows where similarity is higher than a threshold (e.g., 0.1)
high_similarity_matches = df[df['cosine_similarity_sbert'] > 0.1]
print(high_similarity_matches)

